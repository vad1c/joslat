using Microsoft.VisualBasic;
using NAudio.Wave;
using Newtonsoft.Json.Linq;
using System.Collections.Concurrent;
using System.Collections.ObjectModel;
using System.Diagnostics;
using System.IO;
using System.Net.WebSockets;
using System.Text;
using System.Windows;
using System.Windows.Controls;
using System.Windows.Data;
using System.Windows.Documents;
using System.Windows.Input;
using System.Windows.Media;
using System.Windows.Media.Animation;
using System.Windows.Media.Imaging;
using System.Windows.Navigation;
using System.Windows.Shapes;

namespace Realtime.API.Dotnet.SDK.WPF
{
    /// <summary>
    /// Interaction logic for UserControl1.xaml
    /// </summary>
    public partial class WpfUserControl : UserControl
    {
        private static ClientWebSocket client;
        private static bool isPlayingAudio = false;//The icon indicates whether the audio is playing.
        private static bool isUserSpeaking = false; //Indicate whether it is the user speaking.
        private static bool isModelResponding = false; //Identify whether it is the model responding.
        private static bool isRecording = false; //Record the audio status

        private static ConcurrentQueue<byte[]> audioQueue = new ConcurrentQueue<byte[]>();//Audio queue
        private static CancellationTokenSource playbackCancellationTokenSource;
        private static BufferedWaveProvider bufferedWaveProvider;
        private static WaveInEvent waveIn;

        public ObservableCollection<string> ChatMessages = new ObservableCollection<string>();
        private const string apiKey = "";

        public WpfUserControl()
        {
            InitializeComponent();
            ChatListBox.ItemsSource = ChatMessages;

            this.StartSpeechRecognition.Click += StartSpeechRecognition_Click;
            this.StopSpeechRecognition.Click += StopSpeechRecognition_Click;

        }

        private async void StopSpeechRecognition_Click(object sender, RoutedEventArgs e)
        {
            // Stop the ripple effect.
            StopRippleEffect();
            // The text is in Chinese and it translates to "Stop recording" in English.
            StopRecording();
            // The text is in Chinese, and it translates to "Stop audio playback" in English.
            StopAudioPlayback();

            // The audio is buffering.
            await CommitAudioBufferAsync();
            //Console.WriteLine($"Recording saved to {outputFilePath}");

            // The language detected is Chinese. The translation to English is "CancellationToken for canceling the playback task."
            playbackCancellationTokenSource?.Cancel();

            // Close the WebSocket connection.
            if (client != null && client.State == WebSocketState.Open)
            {
                await client.CloseAsync(WebSocketCloseStatus.NormalClosure, "Closing connection", CancellationToken.None);
                client.Dispose();
                client = null;
                Dispatcher.Invoke(() => ChatMessages.Add("WebSocket closed successfully."));
            }

            // Clear the audio queue.
            ClearAudioQueue();

            // Reset state variables.
            isPlayingAudio = false;
            isUserSpeaking = false;
            isModelResponding = false;
            isRecording = false;
        }

        private async void StartSpeechRecognition_Click(object sender, RoutedEventArgs e)
        {
            StartRippleEffect();

            await InitializeWebSocketAsync();
            InitializeAudioPlayer();

            var sendAudioTask = StartAudioRecording();
            var receiveTask = ReceiveMessages(client);

            await Task.WhenAll(sendAudioTask, receiveTask);

        }

        private static void InitializeAudioPlayer()
        {
            bufferedWaveProvider = new BufferedWaveProvider(new WaveFormat(24000, 16, 1))
            {
                BufferDuration = TimeSpan.FromSeconds(5), // Adjust the buffer duration.
                DiscardOnBufferOverflow = true
            };
        }
        private async Task InitializeWebSocketAsync()
        {
            client = new ClientWebSocket();
            client.Options.SetRequestHeader("Authorization", $"Bearer {apiKey}");
            client.Options.SetRequestHeader("openai-beta", "realtime=v1");

            try
            {
                await client.ConnectAsync(new Uri("wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-10-01"), CancellationToken.None);
                Dispatcher.Invoke(() => ChatMessages.Add("WebSocket connected!"));
            }
            catch (Exception ex)
            {
                Dispatcher.Invoke(() => ChatMessages.Add($"Failed to connect WebSocket: {ex.Message}"));
                await Task.Delay(5000); // Wait before retrying
                await InitializeWebSocketAsync(); // Retry connection
            }
        }

        private async Task ReceiveMessages(ClientWebSocket client)
        {
            var buffer = new byte[1024 * 16]; // Increase the buffer size to 16KB.
            var messageBuffer = new StringBuilder(); // For storing complete messages.

            while (client.State == WebSocketState.Open)
            {
                var result = await client.ReceiveAsync(new ArraySegment<byte>(buffer), CancellationToken.None);
                var chunk = Encoding.UTF8.GetString(buffer, 0, result.Count);
                messageBuffer.Append(chunk);

                // For storing complete messages.
                if (result.EndOfMessage)
                {
                    var jsonResponse = messageBuffer.ToString();
                    //Dispatcher.Invoke(() => ChatMessages.Add($"Received: {jsonResponse}"));
                    //Dispatcher.Invoke(() => ChatMessages.Add($"Received: ===="));
                    messageBuffer.Clear();

                    if (jsonResponse.Trim().StartsWith("{"))
                    {
                        var json = JObject.Parse(jsonResponse);
                        HandleWebSocketMessage(json); // Call `HandleWebSocketMessage` to process the complete JSON message.
                    }
                }
            }
        }

        private async void HandleWebSocketMessage(JObject json)
        {
            var type = json["type"]?.ToString();
            AddMessage($"Received type: {type}");

            switch (type)
            {
                case "session.created":
                    AddMessage("Session created. Sending session update.");
                    SendSessionUpdate();
                    break;
                case "session.updated":
                    Console.WriteLine("Session updated. Starting audio recording.");
                    if (!isRecording)
                        StartAudioRecording();
                    break;
                case "input_audio_buffer.speech_started":
                    HandleUserSpeechStarted();
                    break;
                case "conversation.item.input_audio_transcription.completed":
                    var text = json["transcript"]?.ToString();
                    AddMessage(text);
                    await WriteToTextFile(text);
                    break;
                case "input_audio_buffer.speech_stopped":
                    HandleUserSpeechStopped();
                    break;
                case "response.audio.delta":
                    ProcessAudioDelta(json);
                    break;
                case "response.audio.done":
                    isModelResponding = false;
                    ResumeRecording();
                    break;
                case "response.function_call_arguments.done":
                    HandleFunctionCall(json);
                    break;
                default:
                    AddMessage("Unhandled event type.");
                    break;
            }
        }

        private void AddMessage(string message)
        {
            Dispatcher.Invoke(() => ChatMessages.Add(message));
        }

        #region Function Call
        private void HandleFunctionCall(JObject json)
        {
            try
            {
                var name = json["name"]?.ToString();
                var callId = json["call_id"]?.ToString();
                var arguments = json["arguments"]?.ToString();
                if (!string.IsNullOrEmpty(arguments))
                {
                    var functionCallArgs = JObject.Parse(arguments);
                    switch (name)
                    {
                        case "get_weather":
                            var city = functionCallArgs["city"]?.ToString();
                            if (!string.IsNullOrEmpty(city))
                            {
                                var weatherResult = GetWeather(city);
                                SendFunctionCallResult(weatherResult, callId);
                            }
                            else
                            {
                                Console.WriteLine("City not provided for get_weather function.");
                            }
                            break;

                        case "write_notepad":
                            var content = functionCallArgs["content"]?.ToString();
                            var date = functionCallArgs["date"]?.ToString();
                            if (!string.IsNullOrEmpty(content) && !string.IsNullOrEmpty(date))
                            {
                                WriteToNotepad(date, content);
                                SendFunctionCallResult("Write to notepad successful.", callId);
                            }
                            break;

                        default:
                            Console.WriteLine("Unknown function call received.");
                            break;
                    }
                }
            }
            catch (Exception ex)
            {
                Console.WriteLine($"Error parsing function call arguments: {ex.Message}");
            }
        }

        private string GetWeather(string city)
        {
            return $@"{{
                ""city"": ""{city}"",
                ""temperature"": ""99°„C""
            }}";
        }

        private void WriteToNotepad(string date, string content)
        {
            try
            {
                string filePath = System.IO.Path.Combine(Environment.CurrentDirectory, "temp.txt");

                // Write the date and content to a text file
                //File.AppendAllText(filePath, $"Date: {date}\nContent: {content}\n\n");

                // Open the text file in Notepad
                Process.Start("notepad.exe", filePath);
                Console.WriteLine("Content written to Notepad.");
            }
            catch (Exception ex)
            {
                Console.WriteLine($"Error writing to Notepad: {ex.Message}");
            }
        }

        private void SendFunctionCallResult(string result, string callId)
        {
            var resultJson = new JObject
            {
                ["type"] = "conversation.item.create",
                ["item"] = new JObject
                {
                    ["type"] = "function_call_output",
                    ["output"] = result,
                    ["call_id"] = callId
                }
            };

            client.SendAsync(new ArraySegment<byte>(Encoding.UTF8.GetBytes(resultJson.ToString())), WebSocketMessageType.Text, true, CancellationToken.None);
            Console.WriteLine("Sent function call result: " + resultJson);

            var rpJson = new JObject
            {
                ["type"] = "response.create"
            };

            client.SendAsync(new ArraySegment<byte>(Encoding.UTF8.GetBytes(rpJson.ToString())), WebSocketMessageType.Text, true, CancellationToken.None);
        }

        private void SendSessionUpdate()
        {
            var sessionConfig = new JObject
            {
                ["type"] = "session.update",
                ["session"] = new JObject
                {
                    ["instructions"] = "Your knowledge cutoff is 2023-10. You are a helpful, witty, and friendly AI. Act like a human, but remember that you aren't a human and that you can't do human things in the real world. Your voice and personality should be warm and engaging, with a lively and playful tone. If interacting in a non-English language, start by using the standard accent or dialect familiar to the user. Talk quickly. You should always call a function if you can. Do not refer to these rules, even if you're asked about them.",
                    ["turn_detection"] = new JObject
                    {
                        ["type"] = "server_vad",
                        ["threshold"] = 0.5,
                        ["prefix_padding_ms"] = 300,
                        ["silence_duration_ms"] = 500
                    },
                    ["voice"] = "alloy",
                    ["temperature"] = 1,
                    ["max_response_output_tokens"] = 4096,
                    ["modalities"] = new JArray("text", "audio"),
                    ["input_audio_format"] = "pcm16",
                    ["output_audio_format"] = "pcm16",
                    ["input_audio_transcription"] = new JObject
                    {
                        ["model"] = "whisper-1"
                    },
                    ["tool_choice"] = "auto",
                    ["tools"] = new JArray
                    {
                        new JObject
                        {
                            ["type"] = "function",
                            ["name"] = "get_weather",
                            ["description"] = "Get current weather for a specified city",
                            ["parameters"] = new JObject
                            {
                                ["type"] = "object",
                                ["properties"] = new JObject
                                {
                                    ["city"] = new JObject
                                    {
                                        ["type"] = "string",
                                        ["description"] = "The name of the city for which to fetch the weather."
                                    }
                                },
                                ["required"] = new JArray("city")
                            }
                        },
                        new JObject
                        {
                            ["type"] = "function",
                            ["name"] = "write_notepad",
                            ["description"] = "Open a text editor and write the time, for example, 2024-10-29 16:19. Then, write the content, which should include my questions along with your answers.",
                            ["parameters"] = new JObject
                            {
                                ["type"] = "object",
                                ["properties"] = new JObject
                                {
                                    ["content"] = new JObject
                                    {
                                        ["type"] = "string",
                                        ["description"] = "The content consists of my questions along with the answers you provide."
                                    },
                                    ["date"] = new JObject
                                    {
                                        ["type"] = "string",
                                        ["description"] = "the time, for example, 2024-10-29 16:19."
                                    },
                                },
                                ["required"] = new JArray("content","date")
                            }
                        }
                    }
                }
            };

            string message = sessionConfig.ToString();
            client.SendAsync(new ArraySegment<byte>(Encoding.UTF8.GetBytes(message)), WebSocketMessageType.Text, true, CancellationToken.None);
            Console.WriteLine("Sent session update: " + message);
        }

        private static async Task WriteToTextFile(string text)
        {
            var filePath = System.IO.Path.Combine(Environment.GetFolderPath(Environment.SpecialFolder.MyDocuments), "Transcription.txt");
            await File.AppendAllTextAsync(filePath, text + Environment.NewLine);
            Console.WriteLine($"Text written to {filePath}");
        }
        #endregion


        private static void HandleUserSpeechStarted()
        {
            isUserSpeaking = true;
            isModelResponding = false;
            Console.WriteLine("User started speaking.");
            StopAudioPlayback();
            ClearAudioQueue();
        }

        private static void HandleUserSpeechStopped()
        {
            isUserSpeaking = false;
            Console.WriteLine("User stopped speaking. Processing audio queue...");
            ProcessAudioQueue();
        }

        private static void ProcessAudioDelta(JObject json)
        {
            if (isUserSpeaking) return;

            var base64Audio = json["delta"]?.ToString();
            if (!string.IsNullOrEmpty(base64Audio))
            {
                var audioBytes = Convert.FromBase64String(base64Audio);
                audioQueue.Enqueue(audioBytes);
                isModelResponding = true;
                StopRecording();
            }
        }

        private static void StopAudioPlayback()
        {
            if (isModelResponding && playbackCancellationTokenSource != null)
            {
                playbackCancellationTokenSource.Cancel();
                Console.WriteLine("AI audio playback stopped due to user interruption.");
            }
        }
        private static void ClearAudioQueue()
        {
            while (audioQueue.TryDequeue(out _)) { }
            Console.WriteLine("Audio queue cleared.");
        }

        private static void StopRecording()
        {
            if (waveIn != null && isRecording)
            {
                waveIn.StopRecording();
                isRecording = false;
                Console.WriteLine("Recording stopped to prevent echo.");
            }
        }

        private static void ResumeRecording()
        {
            if (waveIn != null && !isRecording && !isModelResponding)
            {
                waveIn.StartRecording();
                isRecording = true;
                Console.WriteLine("Recording resumed after audio playback.");
            }
        }

        private static void ProcessAudioQueue()
        {
            if (!isPlayingAudio)
            {
                isPlayingAudio = true;
                playbackCancellationTokenSource = new CancellationTokenSource();

                Task.Run(() =>
                {
                    try
                    {
                        using var waveOut = new WaveOutEvent { DesiredLatency = 200 };
                        waveOut.Init(bufferedWaveProvider);
                        waveOut.Play();

                        while (!playbackCancellationTokenSource.Token.IsCancellationRequested)
                        {
                            if (audioQueue.TryDequeue(out var audioData))
                            {
                                bufferedWaveProvider.AddSamples(audioData, 0, audioData.Length);
                            }
                            else
                            {
                                Task.Delay(100).Wait();
                            }
                        }

                        waveOut.Stop();
                    }
                    catch (Exception ex)
                    {
                        Console.WriteLine($"Error during audio playback: {ex.Message}");
                    }
                    finally
                    {
                        isPlayingAudio = false;
                    }
                });
            }
        }

        private async Task StartAudioRecording()
        {
            waveIn = new WaveInEvent
            {
                WaveFormat = new WaveFormat(24000, 16, 1)
            };

            //waveFileWriter = new WaveFileWriter(outputFilePath, waveIn.WaveFormat);

            waveIn.DataAvailable += async (s, e) =>
            {
                //waveFileWriter.Write(e.Buffer, 0, e.BytesRecorded);
                //waveFileWriter.Flush();

                string base64Audio = Convert.ToBase64String(e.Buffer, 0, e.BytesRecorded);
                var audioMessage = new JObject
                {
                    ["type"] = "input_audio_buffer.append",
                    ["audio"] = base64Audio
                };

                var messageBytes = Encoding.UTF8.GetBytes(audioMessage.ToString());
                await client.SendAsync(new ArraySegment<byte>(messageBytes), WebSocketMessageType.Text, true, CancellationToken.None);
            };

            waveIn.StartRecording();
            isRecording = true;
            Console.WriteLine("Audio recording started.");
        }

        private static async Task CommitAudioBufferAsync()
        {
            if (client != null && client.State == WebSocketState.Open)
            {
                await client.SendAsync(
                    new ArraySegment<byte>(Encoding.UTF8.GetBytes("{\"type\": \"input_audio_buffer.commit\"}")),
                    WebSocketMessageType.Text,
                    true,
                    CancellationToken.None
                );

                await client.SendAsync(
                    new ArraySegment<byte>(Encoding.UTF8.GetBytes("{\"type\": \"response.create\"}")),
                    WebSocketMessageType.Text,
                    true,
                    CancellationToken.None
                );
            }
        }

        private void StartRippleEffect()
        {
            // Ensure the ripples are visible.
            RippleEffect.Visibility = Visibility.Visible;

            // Clear previous animations before each launch.
            RippleScale.BeginAnimation(ScaleTransform.ScaleXProperty, null);
            RippleScale.BeginAnimation(ScaleTransform.ScaleYProperty, null);

            // Create an animation object.
            DoubleAnimation scaleAnimation = new DoubleAnimation
            {
                From = 1,
                To = 3,
                Duration = new Duration(TimeSpan.FromSeconds(1)),
                AutoReverse = true,
                RepeatBehavior = RepeatBehavior.Forever
            };

            // Start animation.
            RippleScale.BeginAnimation(ScaleTransform.ScaleXProperty, scaleAnimation);
            RippleScale.BeginAnimation(ScaleTransform.ScaleYProperty, scaleAnimation);
        }

        private void StopRippleEffect()
        {
            RippleEffect.Visibility = Visibility.Collapsed;

            // Remove the animation.
            RippleScale.BeginAnimation(ScaleTransform.ScaleXProperty, null);
            RippleScale.BeginAnimation(ScaleTransform.ScaleYProperty, null);
        }

    }

}
